{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks for Data Science Applications\n",
        "## Homework 1: Saliency maps for interpretability\n",
        "\n",
        "**Name**: *\\<insert your name here\\>*\n",
        "\n",
        "**Matricola**: *\\<insert your ID here\\>*\n",
        "\n",
        "> âœ Upload the completed notebook **before 17/11/2023 at 23:59** on the Google Classroom page."
      ],
      "metadata": {
        "id": "BwfXT98e5hQc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V6Ph3UT44Xo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To ensure reproducible results (as much as possible)\n",
        "tf.keras.utils.set_random_seed(1234)"
      ],
      "metadata": {
        "id": "GkYGd_WY_2nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/LM1997610/NN"
      ],
      "metadata": {
        "id": "x9mxBHmTXbSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview\n",
        "\n",
        "Neural networks are powerful tools, but they are **black-boxes**, meaning that it is difficult to provide human-understandable explanations on what they are doing. The field of **explanaibility** is concerned with finding algorithms for achieving this. In this homework, you will be guided in implementing some basic explanaibility algorithms (**saliency maps**), which is an instructive way of playing with the TensorFlow autodiff framework.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. The homework is divided into four mandatory exercises (**5 points in total**), and a few optional exercises. Optional exercises are provided if you like the topic and would like to explore more; you are free to ignore them or complete as many as you want. I will not grade them but I might provide feedback for especially nice solutions.\n",
        "2. Completing the homework successfully will remove 1 exercise from the end-of-term homework.\n",
        "3. If your grade does not satisfy you, you are also free to complete the full EoT homework to recover it.\n",
        "3. The grade can be kept for the entire academic year (up to October 2024).\n",
        "\n",
        "**IMPORTANT - read carefully before starting**:\n",
        "\n",
        "> ðŸŸ¨ *External material*: if you use any external material or inspiration for the code, reference it *explicitly* in the corresponding cell. For the textual descriptions, copy-paste *is not allowed*. <ins>Not following these two points is an immediate 0 mark</ins>.\n",
        "\n",
        "> ðŸ”µ *Grammar*: for the textual descriptions, I will remove points for too many grammatical or textual errors. Please try to be precise and provide nice-to-read descriptions, like if you were writing a report.\n",
        "\n",
        "> ðŸŸ¥ *Vectorization and TensorFlow*: the homework must be done _fully in TensorFlow_ and vectorizing the code as much as possible (e.g., do not loop explicitly over the batch dimension).\n",
        "\n",
        "> ðŸŸª *Math*: you can also use LaTeX in Markdown if you need to write equations or if you need generic math notation."
      ],
      "metadata": {
        "id": "BmzKI83R0uYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Warmup: Data loading\n",
        "\n",
        "For this homework, you can select any **tabular dataset** that you like, for either classification or regression. A few repositories that you can look at:\n",
        "\n",
        "1. The catalog of [TensorFlow Datasets](https://www.tensorflow.org/datasets/).\n",
        "2. The [Kaggle catalog](https://www.kaggle.com/data). For downloading data from Kaggle on Google Colab, you will need to [load your Kaggle authentication token](https://colab.research.google.com/github/corrieann/kaggle/blob/master/kaggle_api_in_colab.ipynb).\n",
        "3. The [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php).\n",
        "4. The [ðŸ¤— HuggingFace Datasets](https://huggingface.co/docs/datasets/) repository.\n",
        "\n",
        "You are not bound to these; any open repository is okay. The choice of dataset will not influence the mark."
      ],
      "metadata": {
        "id": "SE7pCfZK2G5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ **DESCRIPTION OF THE CODE**\n",
        "\n",
        "*Provide a small description of the dataset below (e.g., source, task, bibliographic reference if necessary...), both as text and in the comments of the code.*\n",
        "\n",
        "**TODO**: add description here (1-2 paragraphs)."
      ],
      "metadata": {
        "id": "fzThDtr4VJ5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from NN.Untitled import do_plot, show_two_images, show_images"
      ],
      "metadata": {
        "id": "ryF0jgh-hPnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Insert any data loading code here.\n",
        "# If the data loading part is complex, consider using a separate .py file that you can attach to the submission.\n",
        "\n",
        "(train_data, val_data, test_data), data_info = tfds.load(\"cifar10\",\n",
        "                                                        split=[\"train[0:97%]\", \"train[97%:]\", \"test\"],\n",
        "                                                        batch_size = 200,\n",
        "                                                        as_supervised=True,\n",
        "                                                        with_info = True,\n",
        "                                                        shuffle_files = True)\n",
        "\n",
        "def preproccess(image, label):\n",
        "  image = tf.cast(image, tf.float32)/ 255.0\n",
        "  return image, label\n",
        "\n",
        "train_data = train_data.map(lambda image, label: preproccess(image, label))\n",
        "val_data = val_data.map(lambda image, label: preproccess(image, label))\n",
        "test_data = test_data.map(lambda image, label: preproccess(image, label))\n",
        "\n",
        "# X, y = ..."
      ],
      "metadata": {
        "id": "FpQj4F8n20LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(1 for i in train_data)"
      ],
      "metadata": {
        "id": "qOzHFUCmZdRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1: Train a neural network model (1 point)\n",
        "\n",
        "Define, train, and test a neural network for the provided dataset.\n",
        "\n",
        "â˜ You are free to make any modelling choice (e.g., activation function, normalization layers, etc.), provided the result makes sense.\n",
        "\n",
        "âœ… **Completion requirement**: print on screen the test accuracy of the network. Additional comments and visualizations are also appreciated."
      ],
      "metadata": {
        "id": "5Myy-Aq33upU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = ImageDataGenerator(rotation_range=20,\n",
        "                                        width_shift_range=0.2,\n",
        "                                        height_shift_range=0.2,\n",
        "                                        shear_range=0.2,\n",
        "                                        zoom_range=0.2,\n",
        "                                        horizontal_flip=True,\n",
        "                                        fill_mode='nearest')"
      ],
      "metadata": {
        "id": "krWQQCvPabM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_conv_block(model, n_filters):\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters = n_filters,                 # conv_layer\n",
        "                                kernel_size = (3, 3),\n",
        "                                padding = \"same\"))\n",
        "\n",
        "  model.add(tf.keras.layers.BatchNormalization())                       # batch_normalization\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))  # pool_layer\n",
        "  model.add(tf.keras.layers.Activation('relu'))                         # activation\n",
        "  model.add(tf.keras.layers.Dropout(0.5))                               # drop_out\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "Q6MQVKTMeo7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: define a suitable neural network.\n",
        "\n",
        "hidden_layers =  [128, 256, 512, 512, 512]\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "for i in hidden_layers:\n",
        "  model = add_conv_block(model, i)\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(10))"
      ],
      "metadata": {
        "id": "godyFivmXNNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: train the neural network.\n",
        "\n",
        "lr =  3e-3\n",
        "weight_decay = 0.0001\n",
        "milestones = [4, 6, 8, 10, 12, 13, 14, 16, 18, 19]\n",
        "gamma = 0.75\n",
        "n_epoches = 20\n",
        "\n",
        "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = lr, weight_decay = weight_decay)\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch in milestones:\n",
        "        lr *= gamma\n",
        "    return lr\n",
        "\n",
        "call= [tf.keras.callbacks.TerminateOnNaN(),\n",
        "      tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
        "      tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, verbose=1)]\n",
        "\n",
        "model.compile(optimizer= optimizer, loss = cross_entropy, metrics = accuracy)\n",
        "\n",
        "history = model.fit(train_data, epochs=n_epoches, validation_data = val_data, callbacks = call, verbose=2)"
      ],
      "metadata": {
        "id": "PgB37IKKWuhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if saved load the model :\n",
        "# loaded_model = tf.keras.models.load_model(\"CIFAR10_model.h5\")"
      ],
      "metadata": {
        "id": "VdpEazqXrj1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: test the neural network and print the result on screen.\n",
        "\n",
        "do_plot(history, n_epoches)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_data, verbose=1)\n",
        "print(f\"\\n >> test Loss: {round(test_loss,4)}\\n >> test Accuracy : {round(test_acc, 4)}\")\n",
        "\n",
        "#model.save(\"CIFAR10_model.h5\") # save the model\n",
        "\n",
        "# loaded_model = tf.keras.models.load_model(\"CIFAR10_model.h5\")"
      ],
      "metadata": {
        "id": "cJJFyts8Wqez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2: Computing a vanilla saliency map (1 points)\n",
        "\n",
        "> Before starting, I suggest you read [1] as a warm-up. This is one of the first papers that tried to apply this kind of techniques to modern neural networks.\n",
        "\n",
        "What do we mean by explainability? Consider the neural network $f(\\cdot)$ you just trained, and a prediction $\\hat{y} = f(x)$ we want to analyze. **Feature attribution** methods try to assign a weight $w_i$ to each input feature $x_i$, to understand which parts of the input have contributed the most to the explanation.\n",
        "\n",
        "The simplest feature attribution technique, called **vanilla saliency map**, simply computes the gradient at that point:\n",
        "\n",
        "$$\n",
        "  S(x) = \\left\\lvert \\frac{\\partial f_c(x)}{\\partial x} \\right\\rvert\n",
        "$$\n",
        "\n",
        "where  $c$ is the index corresponding to the predicted class.\n",
        "\n",
        "âœ… **Completion requirement**: Take any point from your test dataset, and compute a saliency map using `tf.GradientTape`. Check the weight to see if you can find anything to \"interpret\". **Note**: I am not evaluating how nice / good the explanation is, only the code."
      ],
      "metadata": {
        "id": "0i1Jr0mQ7lRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Take an element from your test set and compute the saliency map\n",
        "\n",
        "test_images, test_labels = zip(*[(image, label) for image, label in test_data])\n",
        "test_images, test_labels = test_images[0], test_labels[0]\n",
        "\n",
        "def silency_map(index):\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    tensor_input = tf.expand_dims(test_images[index], axis=0)\n",
        "    tape.watch(tensor_input)\n",
        "\n",
        "    class_scores = model(tensor_input, training=False)\n",
        "    predicted_class = np.argmax(class_scores)\n",
        "\n",
        "    class_channel = class_scores[:,predicted_class]\n",
        "\n",
        "  grads = tape.gradient(class_channel, tensor_input)\n",
        "\n",
        "  dgrad_abs = tf.math.abs(grads)\n",
        "  dgrad_max_ = np.max(dgrad_abs, axis=3)[0]\n",
        "\n",
        "  ## normalize to range between 0 and 1\n",
        "  arr_min, arr_max  = np.min(dgrad_max_), np.max(dgrad_max_)\n",
        "  map = (dgrad_max_ - arr_min) / (arr_max - arr_min + 1e-18)\n",
        "\n",
        "  return map, grads"
      ],
      "metadata": {
        "id": "WGeQDlEpJ-3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Check the saliency map to analyze the result. What can you say about the map?\n",
        "\n",
        "img_indexes = [6, 14]\n",
        "\n",
        "for i in img_indexes:\n",
        "  mappa, grad = silency_map(i)\n",
        "  show_two_images(test_images[i], mappa)"
      ],
      "metadata": {
        "id": "e9jp2haA8Uov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3: Advanced saliency maps (1 point)\n",
        "\n",
        "> For this exercise, you can read [2] for an overview on the limits of vanilla saliency maps and a description of SmoothGrad.\n",
        "\n",
        "Saliency maps have several issues: most notably, they suffer from noise and they are not stable to small changes in the input or in the model (try running again the training and interpreting the same point). Many methods have been proposed to overcome this.\n",
        "\n",
        "**[SmoothGrad](https://arxiv.org/abs/1706.03825)**, for example, computes multiple saliency maps from noisy versions of the input:\n",
        "\n",
        "$$\n",
        "  \\text{SmoothGrad}(x) = \\frac{1}{n}\\sum_{i=1}^n S(x + \\varepsilon_i), \\;\\; \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2I)\n",
        "$$\n",
        "\n",
        "where $\\varepsilon$ is a vector of the same shape as $x$, whose values are sampled from a normal distribution with zero mean and small variance.\n",
        "\n",
        "ðŸŸ© **Completion requirement**: Implement the SmoothGrad procedure for the same point. Has the explanation improved? Bonus points if you can avoid running a for-loop, and by calling the gradient operation a single time.\n"
      ],
      "metadata": {
        "id": "XXUhR5ZH9PKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SmoothGrad(index, N = 100, sd = 0.0):\n",
        "\n",
        "  e = tf.random.normal(shape=(N, 32, 32, 3), mean=0.0, stddev=sd)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    tensor_input = test_images[index] + e\n",
        "    tape.watch(tensor_input)\n",
        "\n",
        "    class_scores = model(tensor_input, training=False)\n",
        "    predicted_class = np.argmax(class_scores, axis =1)\n",
        "\n",
        "    # Use tf.stack to create 2D indices for indexing\n",
        "    indices_2d = tf.stack([tf.range(N), predicted_class], axis=-1)\n",
        "\n",
        "    # Use tf.gather_nd to extract values\n",
        "    class_channel = tf.gather_nd(class_scores, indices_2d)\n",
        "\n",
        "  grads = tape.gradient(class_channel, tensor_input)\n",
        "  mean_tensor = tf.reduce_mean(grads, axis=0)\n",
        "\n",
        "  dgrad_abs = tf.math.abs(mean_tensor)\n",
        "  dgrad_max_ = np.max(dgrad_abs, axis=2)\n",
        "\n",
        "  ## normalize to range between 0 and 1\n",
        "  arr_min, arr_max  = np.min(dgrad_max_), np.max(dgrad_max_)\n",
        "  map = (dgrad_max_ - arr_min) / (arr_max - arr_min + 1e-18)\n",
        "\n",
        "  return map"
      ],
      "metadata": {
        "id": "xx6Py_k1I_81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_on_sigma(index, noise_list = [0.0, 0.10, 0.20, 0.40]):\n",
        "\n",
        "    map_list = []\n",
        "    map_list.append(test_images[index])\n",
        "\n",
        "    titles = [\"noise_level :\"] + [str(x)+' %' for x in noise_list]\n",
        "\n",
        "    for noise_level in noise_list:\n",
        "\n",
        "        sigma = noise_level*(tf.reduce_max(test_images[index]) - tf.reduce_min(test_images[index]))\n",
        "\n",
        "        this_map = SmoothGrad(index, N = 100, sd = sigma)\n",
        "\n",
        "        map_list.append(this_map)\n",
        "\n",
        "    return map_list, titles"
      ],
      "metadata": {
        "id": "c_rX4Qzr2_k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_to_show = [4, 26, 50]\n",
        "show_images(images_to_show, evaluate_on_sigma)"
      ],
      "metadata": {
        "id": "1K74rE0ndH_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4: Global explanations (2 points)\n",
        "\n",
        "The previous exercises are examples of **local** explanations, where we try to interpret a single prediction of the network. Sometimes we are interested in **global** explanations, that try to find common patterns of behaviour. Suppose we have a dataset $\\mathcal{T} = \\left\\{x_i\\right\\}$ of examples, we can compute some approximate global measure of influence by averaging their saliency:\n",
        "\n",
        "$$\n",
        "\\text{GlobalSaliency} = \\frac{1}{n} \\sum_i S(x_i)\n",
        "$$\n",
        "\n",
        "To make this exercise more interesting, we will split it into 3 parts.\n",
        "\n",
        "**Exercise 4.1**: write a function to compute in parallel the saliency for multiple examples. Note that the resulting matrix $S$ will have shape $(n, d)$, where $n$ is the number of examples and $d$ the size of the input, which is the Jacobian of the network. Try to write the function by avoiding for-loops and multiple tapes, using the [proper tools from TensorFlow](https://www.tensorflow.org/guide/advanced_autodiff)."
      ],
      "metadata": {
        "id": "pIFUonMOY-Mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Write the required function, possibly avoding for-loops.\n",
        "\n"
      ],
      "metadata": {
        "id": "nJdQ6nbp-rIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4.2**: write a function to compute the global saliency and try to explain the results."
      ],
      "metadata": {
        "id": "As4lalRBcMHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Write the required function."
      ],
      "metadata": {
        "id": "O08JfYJQcS1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4.3**: note that a linear model $f(x) = w^\\top x + b$ is an example of an *intrinsically* interpretable  model, since the weights $w$ can be checked to analyze the global saliency of each feature (see [3])."
      ],
      "metadata": {
        "id": "D2j6pi3XcX86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compare the results you obtained before with a simpler linear model."
      ],
      "metadata": {
        "id": "vjnDxBNicom1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional exercises and parting words\n",
        "\n",
        "> âš  Explainability is a complex topic, with multiple issues arising from the over-abundance of techniques, their instability, etc. While an interesting research topic to pursue, never use blindly these techniques in high-stake applications!\n",
        "\n",
        "These exercises were just a brief and short introduction to the topic of explainability. Below you can find some additional exercises to tackle if you are interested. Remember that these are not part of your grade, but I am happy to provide feedback if they are of interest to you.\n",
        "\n",
        "1. There are dozens of possible variations on feature attribution methods, which may or may not provide better results (see [4] for a benchmarking and this nice [Distill blog post](https://distill.pub/2020/attribution-baselines/)). **[Integrated Gradients](https://arxiv.org/abs/1703.01365)** are an interesting example, where the saliency is integrated over a path ranging from an empty input to the true input. Try implementing integrated gradients.\n",
        "2. **Data attribution** methods are a different class of explanation methods, which try to predict what points in the dataset where most influential to a given prediction (e.g., a picture of a cat will be especially influential on similar pictures). One example of such methods is TracIn [5], which stores checkpoints of the model during training and evaluates the correlation of the gradients. Try to implement TracIn or any other metric of data influence.\n",
        "3. A recent line of research tries to use large language models (e.g., ChatGPT) to explain other models (e.g., see [Language models can explain neurons in language models](https://openai.com/research/language-models-can-explain-neurons-in-language-models)). If you have access to an LLM, you can try it! Take a specific neuron in the model, and collect the activation for multiple examples. Provide these activations to the LLM, and prompt it to provide a human-understandable explanation. What is the result?"
      ],
      "metadata": {
        "id": "Pp2K2VXGdAtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final checklist\n",
        "\n",
        "1. Carefully check all code. Insert comments when needed. Search for \"TODO\" to see if you forgot something.\n",
        "2. Run everything one final time. *Please do not send me notebooks with errors or cells that are not working.*\n",
        "3. Upload the completed notebook **before 17/11/2023 23:59** on the Google Classrom page."
      ],
      "metadata": {
        "id": "GIyU8c7lh4Ly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bibliography\n",
        "\n",
        "[1] Simonyan, K., Vedaldi, A. and Zisserman, A., 2013. [Deep inside convolutional networks: Visualising image classification models and saliency maps](https://arxiv.org/abs/1312.6034). arXiv preprint arXiv:1312.6034.\n",
        "\n",
        "[2] Smilkov, D., Thorat, N., Kim, B., ViÃ©gas, F. and Wattenberg, M., 2017. [SmoothGrad: removing noise by adding noise](https://arxiv.org/abs/1706.03825). arXiv preprint arXiv:1706.03825.\n",
        "\n",
        "[3] Rudin, C., 2019. [Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead](https://www.nature.com/articles/s42256-019-0048-x). Nature Machine Intelligence, 1(5), pp. 206-215.\n",
        "\n",
        "[4] Nguyen, G., Kim, D. and Nguyen, A., 2021. [The effectiveness of feature attribution methods and its correlation with automatic evaluation scores](https://proceedings.neurips.cc/paper/2021/hash/de043a5e421240eb846da8effe472ff1-Abstract.html). Advances in Neural Information Processing Systems, 34, pp.26422-26436.\n",
        "\n",
        "[5] Pruthi, G., Liu, F., Kale, S. and Sundararajan, M., 2020. [Estimating training data influence by tracing gradient descent](https://proceedings.neurips.cc/paper/2020/hash/e6385d39ec9394f2f3a354d9d2b88eec-Abstract.html). Advances in Neural Information Processing Systems, 33, pp. 19920-19930."
      ],
      "metadata": {
        "id": "YCfzjQOIe6CQ"
      }
    }
  ]
}